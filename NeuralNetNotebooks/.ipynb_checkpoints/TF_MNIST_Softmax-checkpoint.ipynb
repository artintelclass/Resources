{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lr = 0.1 # learning rate\n",
    "epochs = 5\n",
    "batch_size = 100\n",
    "# calculate the number of steps based on the number of samples, batch size, and number of epochs\n",
    "num_steps = len(mnist.train.images)/batch_size * epochs \n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784\n",
    "num_classes = 10 \n",
    "\n",
    "# Feature columns describe how to use the input.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[784])] # there is no label, so we simply use x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict({'x':features}), labels.astype(np.int32)))\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(70000).repeat().batch(batch_size)\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(features, labels, mode, params):\n",
    "    #Input layer\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    \n",
    "    #Hidden layers\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "    \n",
    "    # Output layer. Compute logits (1 per class).\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "\n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1) \n",
    "    ## If we're just querying, return \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Compute loss. This method does not work with one hot encoding.\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmp3haq7m\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a201d3790>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmp3haq7m', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(\n",
    "    model_fn=neural_net,\n",
    "    params={\n",
    "        'feature_columns': feature_columns,\n",
    "        'hidden_units': [512, 512],\n",
    "        'n_classes': 10,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmp3haq7m/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3671117, step = 1\n",
      "INFO:tensorflow:global_step/sec: 170.651\n",
      "INFO:tensorflow:loss = 0.34499973, step = 101 (0.587 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.999\n",
      "INFO:tensorflow:loss = 0.2066877, step = 201 (0.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.762\n",
      "INFO:tensorflow:loss = 0.18132208, step = 301 (0.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.596\n",
      "INFO:tensorflow:loss = 0.21603692, step = 401 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.448\n",
      "INFO:tensorflow:loss = 0.15912303, step = 501 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.372\n",
      "INFO:tensorflow:loss = 0.053680953, step = 601 (0.620 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.859\n",
      "INFO:tensorflow:loss = 0.068752766, step = 701 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.316\n",
      "INFO:tensorflow:loss = 0.07211278, step = 801 (0.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.114\n",
      "INFO:tensorflow:loss = 0.17430215, step = 901 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.571\n",
      "INFO:tensorflow:loss = 0.14160278, step = 1001 (0.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.21\n",
      "INFO:tensorflow:loss = 0.13696283, step = 1101 (0.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.858\n",
      "INFO:tensorflow:loss = 0.06491662, step = 1201 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.375\n",
      "INFO:tensorflow:loss = 0.050476313, step = 1301 (0.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.353\n",
      "INFO:tensorflow:loss = 0.051372398, step = 1401 (0.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.47\n",
      "INFO:tensorflow:loss = 0.08991397, step = 1501 (0.557 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.201\n",
      "INFO:tensorflow:loss = 0.112694226, step = 1601 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.877\n",
      "INFO:tensorflow:loss = 0.02883184, step = 1701 (0.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.814\n",
      "INFO:tensorflow:loss = 0.10112599, step = 1801 (0.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.947\n",
      "INFO:tensorflow:loss = 0.11233285, step = 1901 (0.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.194\n",
      "INFO:tensorflow:loss = 0.024599018, step = 2001 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.036\n",
      "INFO:tensorflow:loss = 0.037246708, step = 2101 (0.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.647\n",
      "INFO:tensorflow:loss = 0.022812458, step = 2201 (0.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.979\n",
      "INFO:tensorflow:loss = 0.10101715, step = 2301 (0.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.98\n",
      "INFO:tensorflow:loss = 0.060647544, step = 2401 (0.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.797\n",
      "INFO:tensorflow:loss = 0.030961046, step = 2501 (0.562 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.948\n",
      "INFO:tensorflow:loss = 0.021833675, step = 2601 (0.572 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.411\n",
      "INFO:tensorflow:loss = 0.01618182, step = 2701 (0.573 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2750 into /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmp3haq7m/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.009713469.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x10dd25810>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model.\n",
    "model.train(lambda:training_input_fn(mnist.train.images,mnist.train.labels,batch_size),steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    " \n",
    "    features=dict({'x':features})\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels.astype(np.int32))\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-24-13:51:27\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmp3haq7m/model.ckpt-2750\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-13:51:28\n",
      "INFO:tensorflow:Saving dict for global step 2750: accuracy = 0.9802, global_step = 2750, loss = 0.06205017\n",
      "\n",
      "Test set accuracy: 0.980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_result = model.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(mnist.test.images,mnist.test.labels,batch_size))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmp3haq7m/model.ckpt-2750\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a201bf190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 4\n",
      "Actual Label: 4\n"
     ]
    }
   ],
   "source": [
    "# Predict single image\n",
    "\n",
    "whichImage = random.randint(0,len(mnist.test.images))\n",
    "# Get image from test set\n",
    "test_image = mnist.test.images[whichImage].reshape(1, 784)\n",
    "\n",
    "preds = list(model.predict(input_fn=lambda:eval_input_fn(test_image,None,1)))\n",
    "\n",
    "# Display\n",
    "plt.imshow(np.reshape(test_image, [28, 28]), cmap='gray')\n",
    "plt.show()\n",
    "print(\"Model prediction:\", preds[0]['class_ids'][0])\n",
    "print(\"Actual Label:\",mnist.test.labels[whichImage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
