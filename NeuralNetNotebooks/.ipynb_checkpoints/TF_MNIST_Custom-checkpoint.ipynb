{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lr = 0.1 # learning rate\n",
    "epochs = 6\n",
    "batch_size = 100\n",
    "# calculate the number of steps based on the number of samples, batch size, and number of epochs\n",
    "num_steps = len(mnist.train.images)/batch_size * epochs \n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784\n",
    "num_classes = 10 \n",
    "\n",
    "# Feature columns describe how to use the input.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[784])] # there is no label, so we simply use x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict({'x':features}), labels.astype(np.int32)))\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(70000).repeat().batch(batch_size)\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(features, labels, mode, params):\n",
    "    #Input layer\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    \n",
    "    #Hidden layers\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "    \n",
    "    # Output layer. Compute logits (1 per class).\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "\n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1) \n",
    "    ## If we're just querying, return \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Compute loss.\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpFoE4JV\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a1f3b8590>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpFoE4JV', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(\n",
    "    model_fn=neural_net,\n",
    "    params={\n",
    "        'feature_columns': feature_columns,\n",
    "        'hidden_units': [512, 512],\n",
    "        'n_classes': 10,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpFoE4JV/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.282632, step = 1\n",
      "INFO:tensorflow:global_step/sec: 148.704\n",
      "INFO:tensorflow:loss = 0.2859108, step = 101 (0.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.92\n",
      "INFO:tensorflow:loss = 0.3261734, step = 201 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.332\n",
      "INFO:tensorflow:loss = 0.14059903, step = 301 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.296\n",
      "INFO:tensorflow:loss = 0.07641499, step = 401 (0.661 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.676\n",
      "INFO:tensorflow:loss = 0.118371524, step = 501 (0.583 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.414\n",
      "INFO:tensorflow:loss = 0.05634487, step = 601 (0.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.518\n",
      "INFO:tensorflow:loss = 0.081746906, step = 701 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.241\n",
      "INFO:tensorflow:loss = 0.04350826, step = 801 (0.598 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.201\n",
      "INFO:tensorflow:loss = 0.059295807, step = 901 (0.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.64\n",
      "INFO:tensorflow:loss = 0.057861667, step = 1001 (0.611 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.251\n",
      "INFO:tensorflow:loss = 0.023452697, step = 1101 (0.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.249\n",
      "INFO:tensorflow:loss = 0.066444986, step = 1201 (0.657 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.623\n",
      "INFO:tensorflow:loss = 0.04485665, step = 1301 (0.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.551\n",
      "INFO:tensorflow:loss = 0.1101482, step = 1401 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.819\n",
      "INFO:tensorflow:loss = 0.03937001, step = 1501 (0.600 sec)\n",
      "INFO:tensorflow:global_step/sec: 152.096\n",
      "INFO:tensorflow:loss = 0.028668692, step = 1601 (0.657 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.242\n",
      "INFO:tensorflow:loss = 0.047073297, step = 1701 (0.756 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.92\n",
      "INFO:tensorflow:loss = 0.017308628, step = 1801 (0.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.546\n",
      "INFO:tensorflow:loss = 0.03708192, step = 1901 (0.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.697\n",
      "INFO:tensorflow:loss = 0.02946179, step = 2001 (0.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.983\n",
      "INFO:tensorflow:loss = 0.02111197, step = 2101 (0.720 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.773\n",
      "INFO:tensorflow:loss = 0.012055074, step = 2201 (0.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.525\n",
      "INFO:tensorflow:loss = 0.035022415, step = 2301 (0.604 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.363\n",
      "INFO:tensorflow:loss = 0.013534649, step = 2401 (0.601 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.957\n",
      "INFO:tensorflow:loss = 0.038360782, step = 2501 (0.617 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.64\n",
      "INFO:tensorflow:loss = 0.069491014, step = 2601 (0.647 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.632\n",
      "INFO:tensorflow:loss = 0.012489293, step = 2701 (0.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.585\n",
      "INFO:tensorflow:loss = 0.00832703, step = 2801 (0.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.906\n",
      "INFO:tensorflow:loss = 0.0074943257, step = 2901 (0.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.923\n",
      "INFO:tensorflow:loss = 0.026023345, step = 3001 (0.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.599\n",
      "INFO:tensorflow:loss = 0.022786155, step = 3101 (0.619 sec)\n",
      "INFO:tensorflow:global_step/sec: 163.05\n",
      "INFO:tensorflow:loss = 0.013050613, step = 3201 (0.613 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3300 into /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpFoE4JV/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.01945538.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1a1f3b8390>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model.\n",
    "model.train(lambda:training_input_fn(mnist.train.images,mnist.train.labels,batch_size),steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    " \n",
    "    features=dict({'x':features})\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels.astype(np.int32))\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-24-15:15:46\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpFoE4JV/model.ckpt-3300\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-15:15:47\n",
      "INFO:tensorflow:Saving dict for global step 3300: accuracy = 0.9802, global_step = 3300, loss = 0.06101506\n",
      "\n",
      "Test set accuracy: 98.02%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_result = model.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(mnist.test.images,mnist.test.labels,batch_size))\n",
    "\n",
    "acc=eval_result[\"accuracy\"]\n",
    "print('\\nTest set accuracy: {0:0.2f}%\\n'.format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpFoE4JV/model.ckpt-3300\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1f7f82d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 4\n",
      "Actual Label: 4\n"
     ]
    }
   ],
   "source": [
    "# Predict single image\n",
    "\n",
    "whichImage = random.randint(0,len(mnist.test.images))\n",
    "# Get image from test set\n",
    "test_image = mnist.test.images[whichImage].reshape(1, 784)\n",
    "\n",
    "preds = list(model.predict(input_fn=lambda:eval_input_fn(test_image,None,1)))\n",
    "\n",
    "# Display\n",
    "plt.imshow(np.reshape(test_image, [28, 28]), cmap='gray')\n",
    "plt.show()\n",
    "print(\"Model prediction:\", preds[0]['class_ids'][0])\n",
    "print(\"Actual Label:\",mnist.test.labels[whichImage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
