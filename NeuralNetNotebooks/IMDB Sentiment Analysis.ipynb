{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Sentiment Analysis\n",
    "\n",
    "This dataset has 25,000 moview reviews for training, and another 25,000 reviews for testing. Labels are 0's and 1's. 0 is for a negative review, and 1 is for a postive review. Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "# Notes\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronsherwood/anaconda2/envs/magenta/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 9s 1us/step\n",
      "17473536/17464789 [==============================] - 9s 1us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 102s 4ms/step - loss: 0.4568 - acc: 0.7846 - val_loss: 0.4022 - val_acc: 0.8221\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 101s 4ms/step - loss: 0.3001 - acc: 0.8777 - val_loss: 0.3853 - val_acc: 0.8274\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.2185 - acc: 0.9141 - val_loss: 0.4579 - val_acc: 0.8030\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 113s 5ms/step - loss: 0.1553 - acc: 0.9427 - val_loss: 0.4888 - val_acc: 0.8267\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 116s 5ms/step - loss: 0.1114 - acc: 0.9589 - val_loss: 0.5843 - val_acc: 0.8252\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 120s 5ms/step - loss: 0.0781 - acc: 0.9731 - val_loss: 0.6922 - val_acc: 0.8212\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 118s 5ms/step - loss: 0.0579 - acc: 0.9798 - val_loss: 0.7214 - val_acc: 0.8190\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 118s 5ms/step - loss: 0.0405 - acc: 0.9867 - val_loss: 0.7474 - val_acc: 0.8168\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 119s 5ms/step - loss: 0.0305 - acc: 0.9900 - val_loss: 0.8007 - val_acc: 0.8123\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 125s 5ms/step - loss: 0.0324 - acc: 0.9894 - val_loss: 0.8906 - val_acc: 0.8149\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 124s 5ms/step - loss: 0.0214 - acc: 0.9935 - val_loss: 0.8814 - val_acc: 0.8147\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 124s 5ms/step - loss: 0.0150 - acc: 0.9951 - val_loss: 1.1510 - val_acc: 0.8126\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 117s 5ms/step - loss: 0.0113 - acc: 0.9965 - val_loss: 1.1267 - val_acc: 0.8118\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 116s 5ms/step - loss: 0.0120 - acc: 0.9962 - val_loss: 1.0532 - val_acc: 0.8060\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.0105 - acc: 0.9966 - val_loss: 1.1296 - val_acc: 0.8064\n",
      "25000/25000 [==============================] - 21s 844us/step\n",
      "Test score: 1.1295519102829694\n",
      "Test accuracy: 0.80644\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('imdb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('imdb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go from word index to words setup\n",
    "INDEX_FROM=3\n",
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wonderfully written script br br i praise robert altman this is one of his many films that deals with unconventional fascinating subject matter this film is disturbing but it's sincere and it's sure to elicit a strong emotional response from the viewer if you want to see an unusual film some might even say bizarre this is worth the time br br unfortunately it's very difficult to find in video stores you may have to buy it off the internet\n"
     ]
    }
   ],
   "source": [
    "# Go from word index to words\n",
    "review = x_test[1]\n",
    "print(' '.join(id_to_word[id] for id in review ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this movie was incredible\n"
     ]
    }
   ],
   "source": [
    "# Made up review\n",
    "words = 'this movie was incredible'\n",
    "wordList = re.sub(\"[^\\w]\", \" \",  words).split()\n",
    "wordArray = np.array([[word_to_id[word] if word in word_to_id else 0 for word in wordList]])\n",
    "print(' '.join(id_to_word[id] for id in wordArray[0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment of the review is 97 percent positive\n"
     ]
    }
   ],
   "source": [
    "# Give sentiment as %\n",
    "prediction = model.predict(wordArray)\n",
    "p = prediction[0][0]*100\n",
    "print(\"Sentiment of the review is\",'%.0f' % p,\"percent positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
