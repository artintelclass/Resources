{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lr = 0.1 # learning rate\n",
    "epochs = 6\n",
    "batch_size = 100\n",
    "# calculate the number of steps based on the number of samples, batch size, and number of epochs\n",
    "num_steps = len(mnist.train.images)/batch_size * epochs \n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784\n",
    "num_classes = 10 \n",
    "\n",
    "# Feature columns describe how to use the input.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[784])] # there is no label, so we simply use x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset. Features needs to be a dictionary, labels can be any list or array.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict({'x':features}), labels.astype(np.int32)))\n",
    "    # Shuffle, repeat, and batch the examples. Dataset allows manipulation. \n",
    "    # Choose number larger than dataset to insure complete shuffle (size of buffer)\n",
    "    dataset = dataset.shuffle(70000).repeat().batch(batch_size)\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(features, labels, mode, params):\n",
    "    #Input layer\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    \n",
    "    #Hidden layers\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "    \n",
    "    # Output layer. Compute logits (1 per class).\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "\n",
    "    # Compute predictions.\n",
    "    predicted_classes = tf.argmax(logits, 1) \n",
    "    \n",
    "    # If we're just querying, return \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Compute loss. Sparse is for non-one hot encoded labels\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Compute evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "\n",
    "    # If we're evaulating return evaluation\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN # make sure mode is train\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpbCIECN\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a2ee85e50>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpbCIECN', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(\n",
    "    model_fn=neural_net,\n",
    "    params={\n",
    "        'feature_columns': feature_columns,\n",
    "        'hidden_units': [512, 512],\n",
    "        'n_classes': 10,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpbCIECN/model.ckpt-3300\n",
      "INFO:tensorflow:Saving checkpoints for 3301 into /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpbCIECN/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0055586714, step = 3301\n",
      "INFO:tensorflow:global_step/sec: 165.881\n",
      "INFO:tensorflow:loss = 0.014329302, step = 3401 (0.604 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.228\n",
      "INFO:tensorflow:loss = 0.007609233, step = 3501 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.922\n",
      "INFO:tensorflow:loss = 0.0069104917, step = 3601 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.381\n",
      "INFO:tensorflow:loss = 0.024004655, step = 3701 (0.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.345\n",
      "INFO:tensorflow:loss = 0.032539107, step = 3801 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 148.501\n",
      "INFO:tensorflow:loss = 0.005397578, step = 3901 (0.673 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.453\n",
      "INFO:tensorflow:loss = 0.0136552695, step = 4001 (0.548 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.834\n",
      "INFO:tensorflow:loss = 0.0073472764, step = 4101 (0.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.752\n",
      "INFO:tensorflow:loss = 0.01664187, step = 4201 (0.596 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.804\n",
      "INFO:tensorflow:loss = 0.0031123746, step = 4301 (0.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.238\n",
      "INFO:tensorflow:loss = 0.011375693, step = 4401 (0.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.071\n",
      "INFO:tensorflow:loss = 0.013377073, step = 4501 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.741\n",
      "INFO:tensorflow:loss = 0.0043197507, step = 4601 (0.579 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.55\n",
      "INFO:tensorflow:loss = 0.024366643, step = 4701 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.924\n",
      "INFO:tensorflow:loss = 0.0029043697, step = 4801 (0.667 sec)\n",
      "INFO:tensorflow:global_step/sec: 148.428\n",
      "INFO:tensorflow:loss = 0.01801168, step = 4901 (0.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.418\n",
      "INFO:tensorflow:loss = 0.0030097633, step = 5001 (0.759 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.098\n",
      "INFO:tensorflow:loss = 0.002185696, step = 5101 (0.609 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.065\n",
      "INFO:tensorflow:loss = 0.004517637, step = 5201 (0.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.666\n",
      "INFO:tensorflow:loss = 0.0059964275, step = 5301 (0.579 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.926\n",
      "INFO:tensorflow:loss = 0.003207295, step = 5401 (0.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.46\n",
      "INFO:tensorflow:loss = 0.008289151, step = 5501 (0.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.776\n",
      "INFO:tensorflow:loss = 0.0022301255, step = 5601 (0.566 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.417\n",
      "INFO:tensorflow:loss = 0.00067076535, step = 5701 (0.627 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.651\n",
      "INFO:tensorflow:loss = 0.004261744, step = 5801 (0.585 sec)\n",
      "INFO:tensorflow:global_step/sec: 165.033\n",
      "INFO:tensorflow:loss = 0.0029402515, step = 5901 (0.603 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.822\n",
      "INFO:tensorflow:loss = 0.0025456934, step = 6001 (0.596 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.897\n",
      "INFO:tensorflow:loss = 0.0021365953, step = 6101 (0.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.061\n",
      "INFO:tensorflow:loss = 0.004844796, step = 6201 (0.581 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.613\n",
      "INFO:tensorflow:loss = 0.006314418, step = 6301 (0.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.699\n",
      "INFO:tensorflow:loss = 0.0012793846, step = 6401 (0.634 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.284\n",
      "INFO:tensorflow:loss = 0.0013639043, step = 6501 (0.620 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6600 into /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpbCIECN/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0058383318.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1a2ee960d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model.\n",
    "# Pass the training input function into model.train as a lambda\n",
    "model.train(lambda:training_input_fn(mnist.train.images,mnist.train.labels,batch_size),steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    " \n",
    "    features=dict({'x':features})\n",
    "    if labels is None:\n",
    "        # No labels, use only features. For querying.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels.astype(np.int32))\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-25-13:56:25\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpbCIECN/model.ckpt-6600\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-25-13:56:26\n",
      "INFO:tensorflow:Saving dict for global step 6600: accuracy = 0.9828, global_step = 6600, loss = 0.060031854\n",
      "\n",
      "Test set accuracy: 98.28%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_result = model.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(mnist.test.images,mnist.test.labels,batch_size))\n",
    "\n",
    "# eval_result is a dict\n",
    "acc=eval_result[\"accuracy\"]\n",
    "print('\\nTest set accuracy: {0:0.2f}%\\n'.format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/2z/khrnj_dn3zv_t52wp04myk200000gn/T/tmpbCIECN/model.ckpt-6600\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a23f82350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction: 3\n",
      "Actual Label: 3\n"
     ]
    }
   ],
   "source": [
    "# Predict single image\n",
    "\n",
    "whichImage = random.randint(0,len(mnist.test.images))\n",
    "# Get image from test set\n",
    "test_image = mnist.test.images[whichImage].reshape(1, 784)\n",
    "\n",
    "preds = list(model.predict(input_fn=lambda:eval_input_fn(test_image,None,1)))\n",
    "\n",
    "# Display\n",
    "plt.imshow(np.reshape(test_image, [28, 28]), cmap='gray')\n",
    "plt.show()\n",
    "print(\"Model prediction:\", preds[0]['class_ids'][0])\n",
    "print(\"Actual Label:\",mnist.test.labels[whichImage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
